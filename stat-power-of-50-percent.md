So, in general, the size of the statistical power doesn't matter as much as its having been reported.  The one exception to this is a statistical power around 50%.  A statistical power of  50% is not a bad value in and of itself, but it is the easy answer.  If a researcher has a regression analysis that produced some statistically significant correlations and wants to calculate the statistical power after the fact, the easiest value to use for the minimum correlation to be included is the smallest correlation that the analysis found.  For example, if at a statistical significance of `p<.05` the smallest correlation found had an effect size of `r=0.08`, and the easy thing is to use those values of `0.05` and `0.08` in the calculation of the statistical power.  In which case, the resulting statistical power will be something around 50%.

Of course, a statistical power of 50% doesn't necessarily mean that the research has arbitrarily set the minimum important result equal to the smallest effect size in the results.  Sometimes that simply is the statistical power of an experiment.  But, a reported statistical power of 50% is a good reason to at least double-check the reasoning behind the estimation of what is, or isn't, an important size of effect.

